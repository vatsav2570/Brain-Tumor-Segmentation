{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install monai"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n","!python -c \"import matplotlib\" || pip install -q matplotlib\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install thop"]},{"cell_type":"markdown","metadata":{},"source":["## Setup imports"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","from __future__ import annotations\n","\n","import os\n","import shutil\n","import sys\n","import warnings\n","from collections.abc import Callable, Sequence\n","from pathlib import Path\n","from typing import Any\n","\n","import numpy as np\n","\n","from monai.apps.tcia import (\n","    download_tcia_series_instance,\n","    get_tcia_metadata,\n","    get_tcia_ref_uid,\n","    match_tcia_ref_uid_in_study,\n",")\n","# from monai.apps.utils import download_and_extract\n","from monai.config.type_definitions import PathLike\n","from monai.data import (\n","    CacheDataset,\n","    PydicomReader,\n","    load_decathlon_datalist,\n","    load_decathlon_properties,\n","    partition_dataset,\n","    select_cross_validation_folds,\n",")\n","from monai.transforms import LoadImaged, Randomizable\n","from monai.utils import ensure_tuple\n","\n","def _basename(p: PathLike) -> str:\n","    \"\"\"get the last part of the path (removing the trailing slash if it exists)\"\"\"\n","    sep = os.path.sep + (os.path.altsep or \"\") + \"/ \"\n","    return Path(f\"{p}\".rstrip(sep)).name\n","\n","def extractall(\n","    filepath: PathLike,\n","    output_dir: PathLike = \".\",\n","    hash_val: str | None = None,\n","    hash_type: str = \"md5\",\n","    file_type: str = \"\",\n","    has_base: bool = True,\n",") -> None:\n","    \"\"\"\n","    Extract file to the output directory.\n","    Expected file types are: `zip`, `tar.gz` and `tar`.\n","\n","    Args:\n","        filepath: the file path of compressed file.\n","        output_dir: target directory to save extracted files.\n","        hash_val: expected hash value to validate the compressed file.\n","            if None, skip hash validation.\n","        hash_type: 'md5' or 'sha1', defaults to 'md5'.\n","        file_type: string of file type for decompressing. Leave it empty to infer the type from the filepath basename.\n","        has_base: whether the extracted files have a base folder. This flag is used when checking if the existing\n","            folder is a result of `extractall`, if it is, the extraction is skipped. For example, if A.zip is unzipped\n","            to folder structure `A/*.png`, this flag should be True; if B.zip is unzipped to `*.png`, this flag should\n","            be False.\n","\n","    Raises:\n","        RuntimeError: When the hash validation of the ``filepath`` compressed file fails.\n","        NotImplementedError: When the ``filepath`` file extension is not one of [zip\", \"tar.gz\", \"tar\"].\n","\n","    \"\"\"\n","    if has_base:\n","        # the extracted files will be in this folder\n","        cache_dir = Path(output_dir, _basename(filepath).split(\".\")[0])\n","    else:\n","        cache_dir = Path(output_dir)\n","    if cache_dir.exists() and next(cache_dir.iterdir(), None) is not None:\n","        logger.info(f\"Non-empty folder exists in {cache_dir}, skipped extracting.\")\n","        return\n","    filepath = Path(filepath)\n","    if hash_val and not check_hash(filepath, hash_val, hash_type):\n","        raise RuntimeError(\n","            f\"{hash_type} check of compressed file failed: \" f\"filepath={filepath}, expected {hash_type}={hash_val}.\"\n","        )\n","    print(f\"Writing into directory: {output_dir}.\")\n","    _file_type = file_type.lower().strip()\n","    print(filepath, _file_type)\n","    if filepath.name.endswith(\"zip\") or _file_type == \"zip\":\n","        zip_file = zipfile.ZipFile(filepath)\n","        zip_file.extractall(output_dir)\n","        zip_file.close()\n","        return\n","    if filepath.name.endswith(\"tar\") or filepath.name.endswith(\"tar.gz\") or \"tar\" in _file_type:\n","        tar_file = tarfile.open(filepath)\n","        tar_file.extractall(output_dir)\n","        tar_file.close()\n","        return\n","    raise NotImplementedError(\n","        f'Unsupported file type, available options are: [\"zip\", \"tar.gz\", \"tar\"]. name={filepath} type={file_type}.'\n","    )\n","\n","\n","def download_and_extract(\n","    url: str,\n","    filepath: PathLike = \"\",\n","    output_dir: PathLike = \".\",\n","    hash_val: str | None = None,\n","    hash_type: str = \"md5\",\n","    file_type: str = \"\",\n","    has_base: bool = True,\n","    progress: bool = True,\n",") -> None:\n","    \"\"\"\n","    Download file from URL and extract it to the output directory.\n","\n","    Args:\n","        url: source URL link to download file.\n","        filepath: the file path of the downloaded compressed file.\n","            use this option to keep the directly downloaded compressed file, to avoid further repeated downloads.\n","        output_dir: target directory to save extracted files.\n","            default is the current directory.\n","        hash_val: expected hash value to validate the downloaded file.\n","            if None, skip hash validation.\n","        hash_type: 'md5' or 'sha1', defaults to 'md5'.\n","        file_type: string of file type for decompressing. Leave it empty to infer the type from url's base file name.\n","        has_base: whether the extracted files have a base folder. This flag is used when checking if the existing\n","            folder is a result of `extractall`, if it is, the extraction is skipped. For example, if A.zip is unzipped\n","            to folder structure `A/*.png`, this flag should be True; if B.zip is unzipped to `*.png`, this flag should\n","            be False.\n","        progress: whether to display progress bar.\n","    \"\"\"\n","    print()\n","    with tempfile.TemporaryDirectory() as tmp_dir:\n","        filename = filepath or Path(tmp_dir, _basename(url)).resolve()\n","#         shutil.copy(\"/kaggle/input/segresnet-data/BraTS-MEN-Train.zip\", tmp_dir)\n","#         download_url(url=url, filepath=filename, hash_val=hash_val, hash_type=hash_type, progress=progress)\n","        extractall(filepath=filename, output_dir=output_dir, file_type=file_type, has_base=has_base)\n","        \n","class DecathlonDataset(Randomizable, CacheDataset):\n","    \"\"\"\n","    The Dataset to automatically download the data of Medical Segmentation Decathlon challenge\n","    (http://medicaldecathlon.com/) and generate items for training, validation or test.\n","    It will also load these properties from the JSON config file of dataset. user can call `get_properties()`\n","    to get specified properties or all the properties loaded.\n","    It's based on :py:class:`monai.data.CacheDataset` to accelerate the training process.\n","\n","    Args:\n","        root_dir: user's local directory for caching and loading the MSD datasets.\n","        task: which task to download and execute: one of list (\"Task01_BrainTumour\", \"Task02_Heart\",\n","            \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\",\n","            \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\").\n","        section: expected data section, can be: `training`, `validation` or `test`.\n","        transform: transforms to execute operations on input data.\n","            for further usage, use `EnsureChannelFirstd` to convert the shape to [C, H, W, D].\n","        download: whether to download and extract the Decathlon from resource link, default is False.\n","            if expected file already exists, skip downloading even set it to True.\n","            user can manually copy tar file or dataset folder to the root directory.\n","        val_frac: percentage of validation fraction in the whole dataset, default is 0.2.\n","        seed: random seed to randomly shuffle the datalist before splitting into training and validation, default is 0.\n","            note to set same seed for `training` and `validation` sections.\n","        cache_num: number of items to be cached. Default is `sys.maxsize`.\n","            will take the minimum of (cache_num, data_length x cache_rate, data_length).\n","        cache_rate: percentage of cached data in total, default is 1.0 (cache all).\n","            will take the minimum of (cache_num, data_length x cache_rate, data_length).\n","        num_workers: the number of worker threads if computing cache in the initialization.\n","            If num_workers is None then the number returned by os.cpu_count() is used.\n","            If a value less than 1 is specified, 1 will be used instead.\n","        progress: whether to display a progress bar when downloading dataset and computing the transform cache content.\n","        copy_cache: whether to `deepcopy` the cache content before applying the random transforms,\n","            default to `True`. if the random transforms don't modify the cached content\n","            (for example, randomly crop from the cached image and deepcopy the crop region)\n","            or if every cache item is only used once in a `multi-processing` environment,\n","            may set `copy=False` for better performance.\n","        as_contiguous: whether to convert the cached NumPy array or PyTorch tensor to be contiguous.\n","            it may help improve the performance of following logic.\n","        runtime_cache: whether to compute cache at the runtime, default to `False` to prepare\n","            the cache content at initialization. See: :py:class:`monai.data.CacheDataset`.\n","\n","    Raises:\n","        ValueError: When ``root_dir`` is not a directory.\n","        ValueError: When ``task`` is not one of [\"Task01_BrainTumour\", \"Task02_Heart\",\n","            \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\",\n","            \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\"].\n","        RuntimeError: When ``dataset_dir`` doesn't exist and downloading is not selected (``download=False``).\n","\n","    Example::\n","\n","        transform = Compose(\n","            [\n","                LoadImaged(keys=[\"image\", \"label\"]),\n","                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n","                ScaleIntensityd(keys=\"image\"),\n","                ToTensord(keys=[\"image\", \"label\"]),\n","            ]\n","        )\n","\n","        val_data = DecathlonDataset(\n","            root_dir=\"./\", task=\"Task09_Spleen\", transform=transform, section=\"validation\", seed=12345, download=True\n","        )\n","\n","        print(val_data[0][\"image\"], val_data[0][\"label\"])\n","\n","    \"\"\"\n","\n","    resource = {\n","        \"Task01_BrainTumour\": \"/kaggle/input/segresnet-data/BraTS-MEN-Train\",\n","        \"Task02_Heart\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task02_Heart.tar\",\n","        \"Task03_Liver\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task03_Liver.tar\",\n","        \"Task04_Hippocampus\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task04_Hippocampus.tar\",\n","        \"Task05_Prostate\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task05_Prostate.tar\",\n","        \"Task06_Lung\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task06_Lung.tar\",\n","        \"Task07_Pancreas\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task07_Pancreas.tar\",\n","        \"Task08_HepaticVessel\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task08_HepaticVessel.tar\",\n","        \"Task09_Spleen\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\",\n","        \"Task10_Colon\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task10_Colon.tar\",\n","    }\n","    md5 = {\n","        \"Task01_BrainTumour\": \"240a19d752f0d9e9101544901065d872\",\n","        \"Task02_Heart\": \"06ee59366e1e5124267b774dbd654057\",\n","        \"Task03_Liver\": \"a90ec6c4aa7f6a3d087205e23d4e6397\",\n","        \"Task04_Hippocampus\": \"9d24dba78a72977dbd1d2e110310f31b\",\n","        \"Task05_Prostate\": \"35138f08b1efaef89d7424d2bcc928db\",\n","        \"Task06_Lung\": \"8afd997733c7fc0432f71255ba4e52dc\",\n","        \"Task07_Pancreas\": \"4f7080cfca169fa8066d17ce6eb061e4\",\n","        \"Task08_HepaticVessel\": \"641d79e80ec66453921d997fbf12a29c\",\n","        \"Task09_Spleen\": \"410d4a301da4e5b2f6f86ec3ddba524e\",\n","        \"Task10_Colon\": \"bad7a188931dc2f6acf72b08eb6202d0\",\n","    }\n","\n","    def __init__(\n","        self,\n","        root_dir: PathLike,\n","        task: str,\n","        section: str,\n","        transform: Sequence[Callable] | Callable = (),\n","        download: bool = False,\n","        seed: int = 0,\n","        val_frac: float = 0.2,\n","        cache_num: int = sys.maxsize,\n","        cache_rate: float = 1.0,\n","        num_workers: int = 1,\n","        progress: bool = True,\n","        copy_cache: bool = True,\n","        as_contiguous: bool = True,\n","        runtime_cache: bool = False,\n","    ) -> None:\n","        root_dir = Path(root_dir)\n","        if not root_dir.is_dir():\n","            raise ValueError(\"Root directory root_dir must be a directory.\")\n","        self.section = section\n","        self.val_frac = val_frac\n","        self.set_random_state(seed=seed)\n","        if task not in self.resource:\n","            raise ValueError(f\"Unsupported task: {task}, available options are: {list(self.resource.keys())}.\")\n","        dataset_dir = root_dir / task\n","#         tarfile_name = f\"{dataset_dir}.tar\"\n","#         if download:\n","#             download_and_extract(\n","#                 url=self.resource[task],\n","#                 filepath=tarfile_name,\n","#                 output_dir=root_dir,\n","#                 hash_val=self.md5[task],\n","#                 hash_type=\"md5\",\n","#                 progress=progress,\n","#             )\n","\n","#         if not dataset_dir.exists():\n","#             raise RuntimeError(\n","#                 f\"Cannot find dataset directory: {dataset_dir}, please use download=True to download it.\"\n","#             )\n","#         dataset_dir = \"/kaggle/input/meningits-part1/brain-men-train1\"\n","        self.indices: np.ndarray = np.array([])\n","        data = self._generate_data_list(\"/kaggle/input/segres-json\")\n","        # as `release` key has typo in Task04 config file, ignore it.\n","        property_keys = [\n","            \"name\",\n","            \"description\",\n","            \"reference\",\n","            \"licence\",\n","            \"tensorImageSize\",\n","            \"modality\",\n","            \"labels\",\n","            \"numTraining\",\n","            \"numTest\",\n","        ]\n","#         self._properties = load_decathlon_properties(\"/kaggle/input/segres-json/dataset.json\", property_keys)\n","        if transform == ():\n","            transform = LoadImaged([\"image\", \"label\"])\n","        CacheDataset.__init__(\n","            self,\n","            data=data,\n","            transform=transform,\n","            cache_num=cache_num,\n","            cache_rate=cache_rate,\n","            num_workers=num_workers,\n","            progress=progress,\n","            copy_cache=copy_cache,\n","            as_contiguous=as_contiguous,\n","            runtime_cache=runtime_cache,\n","        )\n","\n","\n","# [docs]\n","    def get_indices(self) -> np.ndarray:\n","        \"\"\"\n","        Get the indices of datalist used in this dataset.\n","\n","        \"\"\"\n","        return self.indices\n","\n","\n","\n","\n","# [docs]\n","    def randomize(self, data: np.ndarray) -> None:\n","        self.R.shuffle(data)\n","\n","\n","\n","\n","# [docs]\n","    def get_properties(self, keys: Sequence[str] | str | None = None) -> dict:\n","        \"\"\"\n","        Get the loaded properties of dataset with specified keys.\n","        If no keys specified, return all the loaded properties.\n","\n","        \"\"\"\n","        if keys is None:\n","            return self._properties\n","        if self._properties is not None:\n","            return {key: self._properties[key] for key in ensure_tuple(keys)}\n","        return {}\n","\n","\n","\n","    def _generate_data_list(self, dataset_dir: PathLike) -> list[dict]:\n","        # the types of the item in data list should be compatible with the dataloader\n","        dataset_dir = Path(dataset_dir) \n","        section = \"training\" if self.section in [\"training\", \"validation\", \"test\"] else \"test\"\n","        datalist = load_decathlon_datalist(\"/kaggle/input/segres-json/dataset.json\", True, section)\n","#         datalist2 = load_decathlon_datalist(\"/kaggle/input/segres-json/dataset.json\", True, section)\n","#         datalist = datalist.append(datalist2)\n","#         print(datalist[898])\n","#         print(\".............................................................................\")\n","        return self._split_datalist(datalist)\n","\n","    def _split_datalist(self, datalist: list[dict]) -> list[dict]:\n","#         if self.section == \"test\":\n","#             return datalist\n","        length = len(datalist)\n","        indices = np.arange(length)\n","        self.randomize(indices)\n","\n","        \n","        val_length = int(length * self.val_frac)\n","        \n","        if self.section == \"training\":\n","            self.indices = indices[val_length:]\n","        elif self.section == \"validation\":\n","            self.indices = indices[:100]\n","        else:\n","            self.indices = indices[100:200]\n","#         print(self.indices)\n","        return [datalist[i] for i in self.indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import tempfile\n","import time\n","import matplotlib.pyplot as plt\n","# from monai.apps import DecathlonDataset\n","from monai.config import print_config\n","from monai.data import DataLoader, decollate_batch\n","from monai.handlers.utils import from_engine\n","from monai.losses import DiceLoss\n","from monai.inferers import sliding_window_inference\n","from monai.metrics import DiceMetric\n","# from monai.networks.nets import SegResNet\n","from monai.transforms import (\n","    Activations,\n","    Activationsd,\n","    AsDiscrete,\n","    AsDiscreted,\n","    Compose,\n","    Invertd,\n","    LoadImaged,\n","    MapTransform,\n","    NormalizeIntensityd,\n","    Orientationd,\n","    RandFlipd,\n","    RandScaleIntensityd,\n","    RandShiftIntensityd,\n","    RandSpatialCropd,\n","    Spacingd,\n","    EnsureTyped,\n","    EnsureChannelFirstd,\n",")\n","from monai.utils import set_determinism\n","\n","import torch\n","\n","print_config()"]},{"cell_type":"markdown","metadata":{},"source":["## Setup data directory\n","\n","You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n","This allows you to save results and reuse downloads.  \n","If not specified a temporary directory will be used."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n","root_dir = tempfile.mkdtemp() if directory is None else directory\n","print(root_dir)"]},{"cell_type":"markdown","metadata":{},"source":["## Set deterministic training for reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["set_determinism(seed=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Define a new transform to convert brain tumor labels\n","\n","Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n","    \"\"\"\n","    Convert labels to multi channels based on brats classes:\n","    label 1 is the peritumoral edema\n","    label 2 is the GD-enhancing tumor\n","    label 3 is the necrotic and non-enhancing tumor core\n","    The possible classes are TC (Tumor core), WT (Whole tumor)\n","    and ET (Enhancing tumor).\n","\n","    \"\"\"\n","\n","    def __call__(self, data):\n","        d = dict(data)\n","        for key in self.keys:\n","            result = []\n","            # merge label 2 and label 3 to construct TC\n","            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n","            # merge labels 1, 2 and 3 to construct WT\n","            result.append(torch.logical_or(torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1))\n","            # label 2 is ET\n","            result.append(d[key] == 2)\n","            d[key] = torch.stack(result, axis=0).float()\n","        return d"]},{"cell_type":"markdown","metadata":{},"source":["## Setup transforms for training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_transform = Compose(\n","    [\n","        # load 4 Nifti images and stack them together\n","        LoadImaged(keys=[\"image\", \"label\"]),\n","        EnsureChannelFirstd(keys=\"image\"),\n","        EnsureTyped(keys=[\"image\", \"label\"]),\n","        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n","        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n","        Spacingd(\n","            keys=[\"image\", \"label\"],\n","            pixdim=(1.0, 1.0, 1.0),\n","            mode=(\"bilinear\", \"nearest\"),\n","        ),\n","        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], random_size=False),\n","        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n","        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n","        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n","        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n","        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n","        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n","    ]\n",")\n","val_transform = Compose(\n","    [\n","        LoadImaged(keys=[\"image\", \"label\"]),\n","        EnsureChannelFirstd(keys=\"image\"),\n","        EnsureTyped(keys=[\"image\", \"label\"]),\n","        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n","        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n","        Spacingd(\n","            keys=[\"image\", \"label\"],\n","            pixdim=(1.0, 1.0, 1.0),\n","            mode=(\"bilinear\", \"nearest\"),\n","        ),\n","        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Quickly load data with DecathlonDataset\n","\n","Here we use `DecathlonDataset` to automatically download and extract the dataset.\n","It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# here we don't cache any data in case out of memory issue\n","train_ds = DecathlonDataset(\n","    root_dir=root_dir,\n","    task=\"Task01_BrainTumour\",\n","    transform=train_transform,\n","    section=\"training\",\n","    download=True,\n","    cache_rate=0.0,\n","    num_workers=4,\n",")\n","train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n","val_ds = DecathlonDataset(\n","    root_dir=root_dir,\n","    task=\"Task01_BrainTumour\",\n","    transform=val_transform,\n","    section=\"validation\",\n","    download=False,\n","    cache_rate=0.0,\n","    num_workers=4,\n",")\n","val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n","\n","test_ds = DecathlonDataset(\n","    root_dir=root_dir,\n","    task=\"Task01_BrainTumour\",\n","    transform=val_transform,\n","    section=\"test\",\n","    download=False,\n","    cache_rate=0.0,\n","    num_workers=4,\n",")\n","test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)\n","\n","print(len(train_ds), len(val_ds), len(test_ds))"]},{"cell_type":"markdown","metadata":{},"source":["## Check data shape and visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pick one image from DecathlonDataset to visualize and check the 4 channels\n","val_data_example = val_ds[2]\n","print(f\"image shape: {val_data_example['image'].shape}\")\n","plt.figure(\"image\", (24, 6))\n","for i in range(4):\n","    plt.subplot(1, 4, i + 1)\n","    plt.title(f\"image channel {i}\")\n","    plt.imshow(val_data_example[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\n","plt.show()\n","# also visualize the 3 channels label corresponding to this image\n","print(f\"label shape: {val_data_example['label'].shape}\")\n","plt.figure(\"label\", (18, 6))\n","for i in range(3):\n","    plt.subplot(1, 3, i + 1)\n","    plt.title(f\"label channel {i}\")\n","    plt.imshow(val_data_example[\"label\"][i, :, :, 60].detach().cpu())\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Create Model, Loss, Optimizer"]},{"cell_type":"markdown","metadata":{},"source":["## Execute a typical PyTorch training process"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Copyright (c) MONAI Consortium\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","from __future__ import annotations\n","\n","import itertools\n","from collections.abc import Sequence\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","from torch.nn import LayerNorm\n","from typing_extensions import Final\n","\n","from monai.networks.blocks import MLPBlock as Mlp\n","from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n","from monai.networks.layers import DropPath, trunc_normal_\n","from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n","from monai.utils.deprecate_utils import deprecated_arg\n","from monai.networks.layers.factories import Act, Conv, Norm, Pool, split_args\n","from monai.networks.blocks import Convolution\n","\n","\n","rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n","\n","__all__ = [\n","    \"SwinUNETR\",\n","    \"window_partition\",\n","    \"window_reverse\",\n","    \"WindowAttention\",\n","    \"SwinTransformerBlock\",\n","    \"PatchMerging\",\n","    \"PatchMergingV2\",\n","    \"MERGING_MODE\",\n","    \"BasicLayer\",\n","    \"SwinTransformer\",\n","]\n","\n","\n","class SwinUNETR(nn.Module):\n","    \"\"\"\n","    Swin UNETR based on: \"Hatamizadeh et al.,\n","    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n","    <https://arxiv.org/abs/2201.01266>\"\n","    \"\"\"\n","\n","    patch_size: Final[int] = 2\n","\n","    @deprecated_arg(\n","        name=\"img_size\",\n","        since=\"1.3\",\n","        removed=\"1.5\",\n","        msg_suffix=\"The img_size argument is not required anymore and \"\n","        \"checks on the input size are run during forward().\",\n","    )\n","    def __init__(\n","        self,\n","        img_size: Sequence[int] | int,\n","        in_channels: int,\n","        out_channels: int,\n","        depths: Sequence[int] = (2, 2, 2, 2),\n","        num_heads: Sequence[int] = (3, 6, 12, 24),\n","        feature_size: int = 24,\n","        norm_name: tuple | str = \"instance\",\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        dropout_path_rate: float = 0.0,\n","        normalize: bool = True,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","        use_v2=False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            img_size: spatial dimension of input image.\n","                This argument is only used for checking that the input image size is divisible by the patch size.\n","                The tensor passed to forward() can have a dynamic shape as long as its spatial dimensions are divisible by 2**5.\n","                It will be removed in an upcoming version.\n","            in_channels: dimension of input channels.\n","            out_channels: dimension of output channels.\n","            feature_size: dimension of network feature size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            norm_name: feature normalization type and arguments.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            dropout_path_rate: drop path rate.\n","            normalize: normalize output intermediate features in each stage.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: number of spatial dims.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","            use_v2: using swinunetr_v2, which adds a residual convolution block at the beggining of each swin stage.\n","\n","        Examples::\n","\n","            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.\n","            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)\n","\n","            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.\n","            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))\n","\n","            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.\n","            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)\n","\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        img_size = ensure_tuple_rep(img_size, spatial_dims)\n","        patch_sizes = ensure_tuple_rep(self.patch_size, spatial_dims)\n","        window_size = ensure_tuple_rep(7, spatial_dims)\n","\n","        if spatial_dims not in (2, 3):\n","            raise ValueError(\"spatial dimension should be 2 or 3.\")\n","\n","        self._check_input_size(img_size)\n","\n","        if not (0 <= drop_rate <= 1):\n","            raise ValueError(\"dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= attn_drop_rate <= 1):\n","            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= dropout_path_rate <= 1):\n","            raise ValueError(\"drop path rate should be between 0 and 1.\")\n","\n","        if feature_size % 12 != 0:\n","            raise ValueError(\"feature_size should be divisible by 12.\")\n","\n","        self.normalize = normalize\n","        \n","\n","        self.swinViT = SwinTransformer(\n","            in_chans=in_channels,\n","            embed_dim=feature_size,\n","            window_size=window_size,\n","            patch_size=patch_sizes,\n","            depths=depths,\n","            num_heads=num_heads,\n","            mlp_ratio=4.0,\n","            qkv_bias=True,\n","            drop_rate=drop_rate,\n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=dropout_path_rate,\n","            norm_layer=nn.LayerNorm,\n","            use_checkpoint=use_checkpoint,\n","            spatial_dims=spatial_dims,\n","            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n","            use_v2=use_v2,\n","        )\n","\n","        self.encoder1 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=in_channels,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","\n","        self.decoder5 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=8 * feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder4 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 8,\n","            out_channels=feature_size * 4,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder3 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 4,\n","            out_channels=feature_size * 2,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","        self.decoder2 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 2,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder1 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n","\n","    def load_from(self, weights):\n","        with torch.no_grad():\n","            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n","            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n","            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers1\")\n","            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers2\")\n","            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers3[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers3\")\n","            self.swinViT.layers3[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers4[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers4\")\n","            self.swinViT.layers4[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.bias\"]\n","            )\n","\n","    @torch.jit.unused\n","    def _check_input_size(self, spatial_shape):\n","        img_size = np.array(spatial_shape)\n","        remainder = (img_size % np.power(self.patch_size, 5)) > 0\n","        if remainder.any():\n","            wrong_dims = (np.where(remainder)[0] + 2).tolist()\n","            raise ValueError(\n","                f\"spatial dimensions {wrong_dims} of input image (spatial shape: {spatial_shape})\"\n","                f\" must be divisible by {self.patch_size}**5.\"\n","            )\n","\n","    def forward(self, x_in):\n","        if not torch.jit.is_scripting():\n","            self._check_input_size(x_in.shape[2:])\n","        \n","        hidden_states_out = self.swinViT(x_in, self.normalize)\n","        enc0 = self.encoder1(x_in)\n","        enc1 = hidden_states_out[0]\n","        enc2 = hidden_states_out[1]\n","        enc3 = hidden_states_out[2]\n","        dec4 = hidden_states_out[4]\n","        dec3 = self.decoder5(dec4, hidden_states_out[3])\n","        dec2 = self.decoder4(dec3, enc3)\n","        dec1 = self.decoder3(dec2, enc2)\n","        dec0 = self.decoder2(dec1, enc1)\n","        out = self.decoder1(dec0, enc0)\n","        logits = self.out(out)\n","        return logits\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"window partition operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        x: input tensor.\n","        window_size: local window size.\n","    \"\"\"\n","    x_shape = x.size()\n","    if len(x_shape) == 5:\n","        b, d, h, w, c = x_shape\n","        x = x.view(\n","            b,\n","            d // window_size[0],\n","            window_size[0],\n","            h // window_size[1],\n","            window_size[1],\n","            w // window_size[2],\n","            window_size[2],\n","            c,\n","        )\n","        windows = (\n","            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n","        )\n","    elif len(x_shape) == 4:\n","        b, h, w, c = x.shape\n","        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n","        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, dims):\n","    \"\"\"window reverse operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        windows: windows tensor.\n","        window_size: local window size.\n","        dims: dimension values.\n","    \"\"\"\n","    if len(dims) == 4:\n","        b, d, h, w = dims\n","        x = windows.view(\n","            b,\n","            d // window_size[0],\n","            h // window_size[1],\n","            w // window_size[2],\n","            window_size[0],\n","            window_size[1],\n","            window_size[2],\n","            -1,\n","        )\n","        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n","\n","    elif len(dims) == 3:\n","        b, h, w = dims\n","        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n","        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n","    return x\n","\n","\n","def get_window_size(x_size, window_size, shift_size=None):\n","    \"\"\"Computing window size based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        x_size: input size.\n","        window_size: local window size.\n","        shift_size: window shifting size.\n","    \"\"\"\n","\n","    use_window_size = list(window_size)\n","    if shift_size is not None:\n","        use_shift_size = list(shift_size)\n","    for i in range(len(x_size)):\n","        if x_size[i] <= window_size[i]:\n","            use_window_size[i] = x_size[i]\n","            if shift_size is not None:\n","                use_shift_size[i] = 0\n","\n","    if shift_size is None:\n","        return tuple(use_window_size)\n","    else:\n","        return tuple(use_window_size), tuple(use_shift_size)\n","\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        qkv_bias: bool = False,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            attn_drop: attention dropout rate.\n","            proj_drop: dropout rate of output.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim**-0.5\n","        mesh_args = torch.meshgrid.__kwdefaults__\n","\n","        if len(self.window_size) == 3:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros(\n","                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n","                    num_heads,\n","                )\n","            )\n","            coords_d = torch.arange(self.window_size[0])\n","            coords_h = torch.arange(self.window_size[1])\n","            coords_w = torch.arange(self.window_size[2])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 2] += self.window_size[2] - 1\n","            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n","            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n","        elif len(self.window_size) == 2:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","            )\n","            coords_h = torch.arange(self.window_size[0])\n","            coords_w = torch.arange(self.window_size[1])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask):\n","        b, n, c = x.shape\n","        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.clone()[:n, :n].reshape(-1)\n","        ].reshape(n, n, -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","        if mask is not None:\n","            nw = mask.shape[0]\n","            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, n, n)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn).to(v.dtype)\n","        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer block based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        shift_size: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        drop_path: float = 0.0,\n","        act_layer: str = \"GELU\",\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            shift_size: window shift size.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            drop_path: stochastic depth rate.\n","            act_layer: activation layer.\n","            norm_layer: normalization layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        self.use_checkpoint = use_checkpoint\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=self.window_size,\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n","\n","    def forward_part1(self, x, mask_matrix):\n","        x_shape = x.size()\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n","            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n","            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = 0\n","            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n","            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","            _, hp, wp, _ = x.shape\n","            dims = [b, hp, wp]\n","\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","        x_windows = window_partition(shifted_x, window_size)\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n","        shifted_x = window_reverse(attn_windows, window_size, dims)\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if len(x_shape) == 5:\n","            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n","                x = x[:, :d, :h, :w, :].contiguous()\n","        elif len(x_shape) == 4:\n","            if pad_r > 0 or pad_b > 0:\n","                x = x[:, :h, :w, :].contiguous()\n","\n","        return x\n","\n","    def forward_part2(self, x):\n","        return self.drop_path(self.mlp(self.norm2(x)))\n","\n","    def load_from(self, weights, n_block, layer):\n","        root = f\"module.{layer}.0.blocks.{n_block}.\"\n","        block_names = [\n","            \"norm1.weight\",\n","            \"norm1.bias\",\n","            \"attn.relative_position_bias_table\",\n","            \"attn.relative_position_index\",\n","            \"attn.qkv.weight\",\n","            \"attn.qkv.bias\",\n","            \"attn.proj.weight\",\n","            \"attn.proj.bias\",\n","            \"norm2.weight\",\n","            \"norm2.bias\",\n","            \"mlp.fc1.weight\",\n","            \"mlp.fc1.bias\",\n","            \"mlp.fc2.weight\",\n","            \"mlp.fc2.bias\",\n","        ]\n","        with torch.no_grad():\n","            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n","            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n","            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n","            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n","            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n","            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n","            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n","            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n","            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n","            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n","            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n","            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n","            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n","            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n","\n","    def forward(self, x, mask_matrix):\n","        shortcut = x\n","        if self.use_checkpoint:\n","            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n","        else:\n","            x = self.forward_part1(x, mask_matrix)\n","        x = shortcut + self.drop_path(x)\n","        if self.use_checkpoint:\n","            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n","        else:\n","            x = x + self.forward_part2(x)\n","        return x\n","\n","\n","class PatchMergingV2(nn.Module):\n","    \"\"\"\n","    Patch merging layer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            norm_layer: normalization layer.\n","            spatial_dims: number of spatial dims.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        if spatial_dims == 3:\n","            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(8 * dim)\n","        elif spatial_dims == 2:\n","            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","            x = torch.cat(\n","                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n","            )\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n","            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","class PatchMerging(PatchMergingV2):\n","    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 4:\n","            return super().forward(x)\n","        if len(x_shape) != 5:\n","            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n","        b, d, h, w, c = x_shape\n","        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","        x0 = x[:, 0::2, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, 0::2, :]\n","        x3 = x[:, 0::2, 0::2, 1::2, :]\n","        x4 = x[:, 1::2, 0::2, 1::2, :]\n","        x5 = x[:, 0::2, 1::2, 0::2, :]\n","        x6 = x[:, 0::2, 0::2, 1::2, :]\n","        x7 = x[:, 1::2, 1::2, 1::2, :]\n","        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n","\n","\n","def compute_mask(dims, window_size, shift_size, device):\n","    \"\"\"Computing region masks based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        dims: dimension values.\n","        window_size: local window size.\n","        shift_size: shift size.\n","        device: device.\n","    \"\"\"\n","\n","    cnt = 0\n","\n","    if len(dims) == 3:\n","        d, h, w = dims\n","        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n","        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n","                    img_mask[:, d, h, w, :] = cnt\n","                    cnt += 1\n","\n","    elif len(dims) == 2:\n","        h, w = dims\n","        img_mask = torch.zeros((1, h, w, 1), device=device)\n","        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","    mask_windows = window_partition(img_mask, window_size)\n","    mask_windows = mask_windows.squeeze(-1)\n","    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","    return attn_mask\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\"\n","    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        depth: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        drop_path: list,\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = False,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        downsample: nn.Module | None = None,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            depth: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            drop_path: stochastic depth rate.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            norm_layer: normalization layer.\n","            downsample: an optional downsampling layer at the end of the layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.window_size = window_size\n","        self.shift_size = tuple(i // 2 for i in window_size)\n","        self.no_shift = tuple(0 for i in window_size)\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","        self.blocks = nn.ModuleList(\n","            [\n","                SwinTransformerBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    window_size=self.window_size,\n","                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                    norm_layer=norm_layer,\n","                    use_checkpoint=use_checkpoint,\n","                )\n","                for i in range(depth)\n","            ]\n","        )\n","        self.downsample = downsample\n","        if callable(self.downsample):\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, c, d, h, w = x_shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c d h w -> b d h w c\")\n","            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n","            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n","            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n","            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, d, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b d h w c -> b c d h w\")\n","\n","        elif len(x_shape) == 4:\n","            b, c, h, w = x_shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c h w -> b h w c\")\n","            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n","            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n","            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b h w c -> b c h w\")\n","        return x\n","\n","\n","class SwinTransformer(nn.Module):\n","    \"\"\"\n","    Swin Transformer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans: int,\n","        embed_dim: int,\n","        window_size: Sequence[int],\n","        patch_size: Sequence[int],\n","        depths: Sequence[int],\n","        num_heads: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        drop_path_rate: float = 0.0,\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        patch_norm: bool = False,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","        use_v2=False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            in_chans: dimension of input channels.\n","            embed_dim: number of linear projection output channels.\n","            window_size: local window size.\n","            patch_size: patch size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            drop_path_rate: stochastic depth rate.\n","            norm_layer: normalization layer.\n","            patch_norm: add normalization after patch embedding.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: spatial dimension.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.patch_norm = patch_norm\n","        self.window_size = window_size\n","        self.patch_size = patch_size\n","        self.patch_embed = PatchEmbed(\n","            patch_size=self.patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n","            spatial_dims=spatial_dims,\n","        )\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","        self.use_v2 = use_v2\n","        self.layers1 = nn.ModuleList()\n","        self.layers2 = nn.ModuleList()\n","        self.layers3 = nn.ModuleList()\n","        self.layers4 = nn.ModuleList()\n","        if self.use_v2:\n","            self.layers1c = nn.ModuleList()\n","            self.layers2c = nn.ModuleList()\n","            self.layers3c = nn.ModuleList()\n","            self.layers4c = nn.ModuleList()\n","        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2**i_layer),\n","                depth=depths[i_layer],\n","                num_heads=num_heads[i_layer],\n","                window_size=self.window_size,\n","                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                norm_layer=norm_layer,\n","                downsample=down_sample_mod,\n","                use_checkpoint=use_checkpoint,\n","            )\n","            if i_layer == 0:\n","                self.layers1.append(layer)\n","            elif i_layer == 1:\n","                self.layers2.append(layer)\n","            elif i_layer == 2:\n","                self.layers3.append(layer)\n","            elif i_layer == 3:\n","                self.layers4.append(layer)\n","            if self.use_v2:\n","                layerc = UnetrBasicBlock(\n","                    spatial_dims=spatial_dims,\n","                    in_channels=embed_dim * 2**i_layer,\n","                    out_channels=embed_dim * 2**i_layer,\n","                    kernel_size=3,\n","                    stride=1,\n","                    norm_name=\"instance\",\n","                    res_block=True,\n","                )\n","                if i_layer == 0:\n","                    self.layers1c.append(layerc)\n","                elif i_layer == 1:\n","                    self.layers2c.append(layerc)\n","                elif i_layer == 2:\n","                    self.layers3c.append(layerc)\n","                elif i_layer == 3:\n","                    self.layers4c.append(layerc)\n","\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","\n","    def proj_out(self, x, normalize=False):\n","        if normalize:\n","            x_shape = x.size()\n","            if len(x_shape) == 5:\n","                n, ch, d, h, w = x_shape\n","                x = rearrange(x, \"n c d h w -> n d h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n d h w c -> n c d h w\")\n","            elif len(x_shape) == 4:\n","                n, ch, h, w = x_shape\n","                x = rearrange(x, \"n c h w -> n h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n h w c -> n c h w\")\n","        return x\n","\n","    def forward(self, x, normalize=True):\n","        x0 = self.patch_embed(x)\n","        x0 = self.pos_drop(x0)\n","        x0_out = self.proj_out(x0, normalize)\n","        if self.use_v2:\n","            x0 = self.layers1c[0](x0.contiguous())\n","        x1 = self.layers1[0](x0.contiguous())\n","        x1_out = self.proj_out(x1, normalize)\n","        if self.use_v2:\n","            x1 = self.layers2c[0](x1.contiguous())\n","        x2 = self.layers2[0](x1.contiguous())\n","        x2_out = self.proj_out(x2, normalize)\n","        if self.use_v2:\n","            x2 = self.layers3c[0](x2.contiguous())\n","        x3 = self.layers3[0](x2.contiguous())\n","        x3_out = self.proj_out(x3, normalize)\n","        if self.use_v2:\n","            x3 = self.layers4c[0](x3.contiguous())\n","        x4 = self.layers4[0](x3.contiguous())\n","        x4_out = self.proj_out(x4, normalize)\n","        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n","\n","\n","def filter_swinunetr(key, value):\n","    \"\"\"\n","    A filter function used to filter the pretrained weights from [1], then the weights can be loaded into MONAI SwinUNETR Model.\n","    This function is typically used with `monai.networks.copy_model_state`\n","    [1] \"Valanarasu JM et al., Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training\n","    <https://arxiv.org/abs/2307.16896>\"\n","\n","    Args:\n","        key: the key in the source state dict used for the update.\n","        value: the value in the source state dict used for the update.\n","\n","    Examples::\n","\n","        import torch\n","        from monai.apps import download_url\n","        from monai.networks.utils import copy_model_state\n","        from monai.networks.nets.swin_unetr import SwinUNETR, filter_swinunetr\n","\n","        model = SwinUNETR(img_size=(96, 96, 96), in_channels=1, out_channels=3, feature_size=48)\n","        resource = (\n","            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n","        )\n","        ssl_weights_path = \"./ssl_pretrained_weights.pth\"\n","        download_url(resource, ssl_weights_path)\n","        ssl_weights = torch.load(ssl_weights_path)[\"model\"]\n","\n","        dst_dict, loaded, not_loaded = copy_model_state(model, ssl_weights, filter_func=filter_swinunetr)\n","\n","    \"\"\"\n","    if key in [\n","        \"encoder.mask_token\",\n","        \"encoder.norm.weight\",\n","        \"encoder.norm.bias\",\n","        \"out.conv.conv.weight\",\n","        \"out.conv.conv.bias\",\n","    ]:\n","        return None\n","\n","    if key[:8] == \"encoder.\":\n","        if key[8:19] == \"patch_embed\":\n","            new_key = \"swinViT.\" + key[8:]\n","        else:\n","            new_key = \"swinViT.\" + key[8:18] + key[20:]\n","\n","        return new_key, value\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["max_epochs = 0\n","val_interval = 1\n","VAL_AMP = True\n","\n","# standard PyTorch program style: create VNet, DiceLoss and Adam optimizer\n","device = torch.device(\"cuda:0\")\n","from monai.networks.nets import VNet\n","from thop import profile\n","from thop import clever_format\n","from monai.losses import DiceLoss\n","from monai.metrics import DiceMetric\n","from monai.transforms import Compose, Activations, AsDiscrete\n","from monai.inferers import sliding_window_inference\n","\n","image_size = 128\n","\n","# Instantiate the model\n","model = SwinUNETR(img_size=(image_size, image_size, image_size), in_channels=4, out_channels=3, feature_size=48)\n","\n","# # Generate random inputs\n","# inputs = torch.rand(1, 4, image_size, image_size, image_size)\n","\n","# # Perform a forward pass\n","# outputs = model(inputs)\n","# print(outputs.shape)\n","\n","# Calculate FLOPs and parameters\n","# flops, params = profile(model, inputs=(inputs,))\n","# flops, params = clever_format([flops, params], \"%.2f\")\n","# print(f\"FLOPs: {flops}, Params: {params}\")\n","\n","# Define loss function\n","loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n","\n","# Define optimizer\n","optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n","\n","# Define learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n","\n","# Define Dice metric\n","dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n","dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n","\n","# Define post-processing transformations\n","post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n","\n","# Define inference method\n","def inference(input):\n","    def _compute(input):\n","        return sliding_window_inference(\n","            inputs=input,\n","            roi_size=(128, 128, 128),\n","            sw_batch_size=1,\n","            predictor=model,\n","            overlap=0.5,\n","        )\n","\n","    if VAL_AMP:\n","        with torch.cuda.amp.autocast():\n","            return _compute(input)\n","    else:\n","        return _compute(input)\n","\n","# Use AMP to accelerate training\n","scaler = torch.cuda.amp.GradScaler()\n","\n","# Enable cuDNN benchmark\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torchsummary import summary\n","\n","# Assuming the model is named 'model'\n","summary(model, input_size=(128, 128, 128))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install torchsummary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from torchsummary import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_size = [1, 2, 128, 128, 128]  # Example input size\n","\n","# Print the model summary\n","summary(model, input_size=input_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["best_metric = -1\n","best_metric_epoch = -1\n","best_metrics_epochs_and_time = [[], [], []]\n","epoch_loss_values = []\n","val_epoch_loss_values = []\n","metric_values = []\n","train_metric_values = []\n","metric_values_tc = []\n","metric_values_wt = []\n","metric_values_et = []\n","\n","start_epoch = 0\n","latest_checkpoint_path = \"/kaggle/input/no-epoch-48/NoEncoder_best_metric_model_48.pth\"\n","if os.path.exists(latest_checkpoint_path):\n","    checkpoint = torch.load(latest_checkpoint_path)\n","    model.load_state_dict(checkpoint)\n","    start_epoch = 49\n","    print(\"checkpoint loaded\")\n","else:\n","    print(\"No checkpoint. Starting from scratch\")\n","    \n","end_epoch = start_epoch + max_epochs\n","\n","total_start = time.time()\n","model.to(device)\n","for epoch in range(start_epoch, end_epoch):\n","    epoch_start = time.time()\n","    print(\"-\" * 10)\n","    print(f\"epoch {epoch + 1}/{end_epoch}\")\n","    model.train()\n","    epoch_loss = 0\n","    epoch_metric = 0\n","    val_epoch_loss = 0\n","    step = 0\n","    for batch_data in train_loader:\n","        step_start = time.time()\n","        step += 1\n","        inputs, labels = (\n","            batch_data[\"image\"].to(device),\n","            batch_data[\"label\"].to(device),\n","        )\n","        optimizer.zero_grad()\n","        with torch.cuda.amp.autocast():\n","            outputs = model(inputs)\n","            loss = loss_function(outputs, labels)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        epoch_loss += loss.item()\n","        outputs = [post_trans(i) for i in decollate_batch(outputs)]\n","        dice_metric(y_pred=outputs, y=labels)\n","        train_metric = dice_metric.aggregate().item()\n","        epoch_metric += train_metric\n","        \n","#         dice_metric_batch(y_pred=outputs, y=labels)\n","        print(\n","            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n","            f\", train_loss: {loss.item():.4f}\"\n","            f\", current mean dice: {train_metric:.4f}\"\n","            f\", step time: {(time.time() - step_start):.4f}\"\n","        )\n","    lr_scheduler.step()\n","    epoch_loss /= step\n","    epoch_metric /= step\n","    epoch_loss_values.append(epoch_loss)\n","    train_metric_values.append(epoch_metric)\n","    print(\"train time: \", time.time()-epoch_start)\n","    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\"\n","          f\", average metric: {epoch_metric:.4f}\")\n","\n","    if (epoch + 1) % val_interval == 0:\n","        val_start = time.time()\n","        model.eval()\n","        with torch.no_grad():\n","            val_step = 0\n","            for val_data in val_loader:\n","                val_step += 1\n","                val_inputs, val_labels = (\n","                    val_data[\"image\"].to(device),\n","                    val_data[\"label\"].to(device),\n","                )\n","                val_outputs = inference(val_inputs)\n","                loss = loss_function(val_outputs, val_labels)\n","                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n","                dice_metric(y_pred=val_outputs, y=val_labels)\n","                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n","                val_epoch_loss += loss.item()\n","                \n","            metric = dice_metric.aggregate().item()\n","            metric_values.append(metric)\n","            metric_batch = dice_metric_batch.aggregate()\n","            metric_tc = metric_batch[0].item()\n","            metric_values_tc.append(metric_tc)\n","            metric_wt = metric_batch[1].item()\n","            metric_values_wt.append(metric_wt)\n","            metric_et = metric_batch[2].item()\n","            metric_values_et.append(metric_et)\n","            dice_metric.reset()\n","            dice_metric_batch.reset()\n","\n","            if metric > best_metric:\n","                best_metric = metric\n","                best_metric_epoch = epoch + 1\n","                best_metrics_epochs_and_time[0].append(best_metric)\n","                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n","                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n","                torch.save(\n","                    model.state_dict(),\n","                    os.path.join(\"/kaggle/working/\", f\"NoEncoder_best_metric_model_{epoch+1}.pth\"),\n","                )\n","                print(\"saved new best metric model\")\n","            chk_file_name = \"unetr_epoch_\" + str(epoch+1) + \"_model.pth\"    \n","            torch.save(\n","                    model.state_dict(),\n","                    os.path.join(\"/kaggle/working/\", chk_file_name),\n","                )\n","            val_epoch_loss /= val_step\n","            val_epoch_loss_values.append(val_epoch_loss)\n","            print(\n","                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n","                f\", val_loss: {val_epoch_loss:.4f}\"\n","                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n","                f\"\\nbest mean dice: {best_metric:.4f}\"\n","                f\" at epoch: {best_metric_epoch}\"\n","            )\n","        print(\"val time: \", time.time()-val_start)\n","    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n","total_time = time.time() - total_start"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(len(val_epoch_loss_values), val_epoch_loss_values)\n","print(len(epoch_loss_values), epoch_loss_values)\n","print(len(train_metric_values), train_metric_values)\n","print(len(metric_values), metric_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import shutil\n","\n","# source_path = \"/kaggle/input/metrics-loss/SegResNet_data.csv\"\n","# destination_path = \"/kaggle/working/UNETR_data.csv\"\n","\n","# # Copy the file\n","# shutil.copy(source_path, destination_path)\n","\n","# print(f\"File '{source_path}' copied to '{destination_path}'.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import csv\n","\n","# Your lists of data\n","# list1 = [1, 2, 3, 4, 5]\n","# list2 = ['a', 'b', 'c', 'd', 'e']\n","# list3 = [10, 20, 30, 40, 50]\n","# list4 = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n","\n","# CSV file path\n","csv_file_path = '/kaggle/working/UNETR_data.csv'\n","\n","# Column names\n","fieldnames = ['epoch', 'epoch_loss_values', 'train_metric_values', 'val_epoch_loss_values', 'metric_values']\n","\n","# Writing lists to a CSV file with specific column names\n","with open(csv_file_path, 'w', newline='') as csvfile:  # Change 'w' to 'a' for append mode\n","    # Create a CSV writer object with DictWriter\n","    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    csv_writer.writeheader()\n","    # Write the lists to the specified columns\n","    for i in range(len(epoch_loss_values)):\n","        csv_writer.writerow({\n","            'epoch': start_epoch + i + 1,\n","            'epoch_loss_values': epoch_loss_values[i],\n","            'train_metric_values': train_metric_values[i],\n","            'val_epoch_loss_values': val_epoch_loss_values[i],\n","            'metric_values': metric_values[i]\n","        })\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Plot the loss and metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(\"train\", (12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Epoch Average Loss\")\n","x = [i + 1 for i in range(len(epoch_loss_values))]\n","y = epoch_loss_values\n","plt.xlabel(\"epoch\")\n","plt.plot(x, y, color=\"red\")\n","plt.subplot(1, 2, 2)\n","plt.title(\"Val Mean Dice\")\n","x = [val_interval * (i + 1) for i in range(len(metric_values))]\n","y = metric_values\n","plt.xlabel(\"epoch\")\n","plt.plot(x, y, color=\"green\")\n","plt.show()\n","\n","plt.figure(\"train\", (18, 6))\n","plt.subplot(1, 3, 1)\n","plt.title(\"Val Mean Dice TC\")\n","x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n","y = metric_values_tc\n","plt.xlabel(\"epoch\")\n","plt.plot(x, y, color=\"blue\")\n","plt.subplot(1, 3, 2)\n","plt.title(\"Val Mean Dice WT\")\n","x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n","y = metric_values_wt\n","plt.xlabel(\"epoch\")\n","plt.plot(x, y, color=\"brown\")\n","plt.subplot(1, 3, 3)\n","plt.title(\"Val Mean Dice ET\")\n","x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n","y = metric_values_et\n","plt.xlabel(\"epoch\")\n","plt.plot(x, y, color=\"purple\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Check best model output with the input image and label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install nibabel"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import nibabel as nib \n","# img_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t1c.nii\"\n","# label_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t1n.nii\"\n","# img_add_2 = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2f.nii\"\n","# img_add_3 = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2w.nii\"\n","# img = nib.load(img_add).get_fdata()\n","# label = nib.load(label_add).get_fdata()\n","# img_2 = nib.load(img_add_2).get_fdata()\n","# img_3 = nib.load(img_add_3).get_fdata()\n","# print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n","# plt.figure(\"image\", (18, 6))\n","# plt.subplot(1, 4, 1)\n","# plt.title(\"BraTS-GLI-00814-000-t1c.nii\")\n","# plt.imshow(img[:, :, 78], cmap=\"gray\")\n","# plt.subplot(1, 4, 2)\n","# plt.title(\"BraTS-GLI-00814-000-t1n.nii\")\n","# plt.imshow(label[:, :, 78], cmap=\"gray\")\n","# plt.subplot(1, 4, 3)\n","# plt.title(\"BraTS-GLI-00814-000-t2f.nii\")\n","# plt.imshow(img_2[:, :, 78], cmap=\"gray\")\n","# plt.subplot(1, 4, 4)\n","# plt.title(\"BraTS-GLI-00814-000-t2w.nii\")\n","# plt.imshow(img_3[:, :, 78], cmap=\"gray\")\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import nibabel as nib \n","from matplotlib.colors import ListedColormap\n","# img_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2f.nii\"\n","# label_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-seg.nii\"\n","# img = nib.load(img_add).get_fdata()\n","# label = nib.load(label_add).get_fdata()\n","# # Create a custom colormap with purple, blue, and orange colors\n","# cmap = ListedColormap(['black', 'blue', 'orange', 'green'])\n","\n","# print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n","# plt.figure(\"image\", (18, 6))\n","# plt.subplot(1, 3, 1)\n","# plt.title(\"BraTS-GLI-00000-000-t2f.nii\")\n","# plt.imshow(img[:, :, 78], cmap=\"gray\")\n","# plt.subplot(1, 3, 2)\n","# plt.title(\"BraTS-GLI-00000-000-seg.nii\")\n","# plt.imshow(label[:, :, 78])\n","# plt.subplot(1, 3, 3)\n","# plt.title(\"BraTS-GLI-00000-000-seg.nii\")\n","# plt.imshow(label[:, :, 78], cmap = cmap)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load(\"/kaggle/input/no-epoch-48/NoEncoder_best_metric_model_48.pth\"))\n","cmap = ListedColormap(['black', 'yellow', 'blue', 'red'])\n","model.eval()\n","with torch.no_grad():\n","    # select one image to evaluate and visualize the model output\n","    val_input = val_ds[1][\"image\"].unsqueeze(0).to(device)\n","    roi_size = (128, 128, 128)\n","    sw_batch_size = 4\n","    val_output = inference(val_input)\n","    val_output = post_trans(val_output[0])\n","    titles_modalities = [\"t1c\", \"t1n\", \"t2f\", \"t2w\"]\n","    titles = [\"Tumor Core (TC) i/p Channel\", \"Whole Tumor (WT) i/p Channel\", \"Enhancing tumor (ET) i/p Channel\"]\n","    plt.figure(\"image\", (24, 6))\n","    for i in range(4):\n","        plt.subplot(1, 4, i + 1)\n","        plt.title(titles_modalities[i])\n","        plt.imshow(val_ds[1][\"image\"][i, :, :, 78].detach().cpu(), cmap=\"gray\")\n","    plt.show()\n","#     # visualize the 3 channels label corresponding to this image\n","#     plt.figure(\"label\", (18, 6))\n","#     for i in range(3):\n","#         plt.subplot(1, 3, i + 1)\n","#         plt.title(titles[i])\n","#         plt.imshow(val_ds[1][\"label\"][i, :, :, 78].detach().cpu())\n","#     plt.show()\n","#     # visualize the 3 channels model output corresponding to this image\n","#     plt.figure(\"output\", (18, 6))\n","#     for i in range(3):\n","#         plt.subplot(1, 3, i + 1)\n","#         plt.title(f\"output channel {i}\")\n","#         plt.imshow(val_output[i, :, :, 78].detach().cpu())\n","#     plt.show()\n","    \n","    seg_label = torch.zeros((val_ds[1][\"label\"].shape[1], val_ds[1][\"label\"].shape[2], val_ds[1][\"label\"].shape[3]))\n","    seg_label[val_ds[1][\"label\"][1] == 1] = 2\n","    seg_label[val_ds[1][\"label\"][0] == 1] = 1\n","    seg_label[val_ds[1][\"label\"][2] == 1] = 4\n","    \n","    seg_out = torch.zeros((val_output.shape[1], val_output.shape[2], val_output.shape[3]))\n","    seg_out[val_output[1] == 1] = 2\n","    seg_out[val_output[0] == 1] = 1\n","    seg_out[val_output[2] == 1] = 4\n","    \n","    slice_num = 78\n","#     img_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00742-000/BraTS-GLI-00742-000-t2w.nii\",\n","#     label_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00742-000/BraTS-GLI-00742-000-seg.nii\",\n","#     img = nib.load(img_add).get_fdata()\n","#     label = nib.load(label_add).get_fdata()\n","#     plt.figure(\"image\", (18, 6))\n","# #     plt.subplot(1, 3, 1)\n","# #     plt.title(\"image\")\n","# #     plt.imshow(seg_label[:, :, slice_num])\n","#     plt.subplot(1, 2, 1)\n","#     plt.title(\"Ground Truth Mask\")\n","#     plt.imshow(seg_label[:, :, slice_num])\n","#     plt.subplot(1, 2, 2)\n","#     plt.title(\"Predicted Mask\")\n","#     plt.imshow(seg_out[:, :, slice_num])\n","#     plt.show()\n","    \n","    plt.figure(\"image\", (18, 6))\n","    for i in range(3):\n","        plt.subplot(1, 4, i + 1)\n","        plt.title(titles[i])\n","        plt.imshow(val_ds[1][\"label\"][i, :, :, 78].detach().cpu())\n","    plt.subplot(1, 4, 4)\n","    plt.title(\"Ground Truth Mask\")\n","    plt.imshow(seg_label[:, :, slice_num])\n","    plt.show()   \n","    titles_output = [\"Tumor Core (TC) o/p Channel\", \"Whole Tumor (WT) o/p Channel\", \"Enhancing tumor (ET) o/p Channel\"]\n","    plt.figure(\"image\", (18, 6))\n","    for i in range(3):\n","        plt.subplot(1, 4, i + 1)\n","        plt.title(titles_output[i])\n","        plt.imshow(val_output[i, :, :, 78].detach().cpu())\n","    plt.subplot(1, 4, 4)\n","    plt.title(\"UNETR\")\n","    plt.imshow(seg_out[:, :, slice_num], cmap = cmap)\n","    plt.show() "]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation on original image spacings"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["val_org_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\", \"label\"]),\n","        EnsureChannelFirstd(keys=[\"image\"]),\n","        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n","        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n","        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n","        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n","    ]\n",")\n","\n","post_transforms = Compose(\n","    [\n","        Invertd(\n","            keys=\"pred\",\n","            transform=val_transform,\n","            orig_keys=\"image\",\n","            meta_keys=\"pred_meta_dict\",\n","            orig_meta_keys=\"image_meta_dict\",\n","            meta_key_postfix=\"meta_dict\",\n","            nearest_interp=False,\n","            to_tensor=True,\n","            device=\"cpu\",\n","        ),\n","        Activationsd(keys=\"pred\", sigmoid=True),\n","        AsDiscreted(keys=\"pred\", threshold=0.5),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from monai.metrics import HausdorffDistanceMetric\n","from monai.metrics import SurfaceDiceMetric\n","from monai.metrics import MeanIoU\n","from monai.metrics import SurfaceDistanceMetric\n","from monai.metrics import ROCAUCMetric\n","from monai.metrics import SurfaceDiceMetric\n","\n","hd_metric = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile = 95)\n","hd_metric_batch = HausdorffDistanceMetric(include_background=True, reduction=\"mean_batch\", percentile = 95)\n","\n","sd_metric = SurfaceDistanceMetric(include_background=True, reduction=\"mean\")\n","sd_metric_batch = SurfaceDistanceMetric(include_background=True, reduction=\"mean_batch\")\n","\n","meanIoU_metric = MeanIoU(include_background=True, reduction=\"mean\")\n","meanIoU_metric_batch = MeanIoU(include_background=True, reduction=\"mean_batch\")\n","\n","surfaceDice_metric = SurfaceDiceMetric(include_background=True, reduction=\"mean\", class_thresholds = (0.01, 0.01, 0.01))\n","surfaceDice_metric_batch = SurfaceDiceMetric(include_background=True, reduction=\"mean_batch\", class_thresholds = (0.01, 0.01, 0.01))\n","\n","model.load_state_dict(torch.load(\"/kaggle/input/no-epoch-48/NoEncoder_best_metric_model_48.pth\"))\n","model.eval()\n","count = 0\n","infer_start=time.time()\n","with torch.no_grad():\n","    for val_data in test_loader:\n","        count = count + 1\n","        print(count)\n","#         val_inputs = val_data[\"image\"].to(device)\n","#         val_data[\"pred\"] = inference(val_inputs)\n","#         val_data = [post_trans(i) for i in decollate_batch(val_data)]\n","#         val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n","        val_inputs, val_labels = (\n","            val_data[\"image\"].to(device),\n","            val_data[\"label\"].to(device),\n","        )\n","        inf_time = time.time()\n","        val_outputs = inference(val_inputs)\n","        print(\"inf time: \", time.time()-inf_time)\n","        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n","        \n","        dice_metric(y_pred=val_outputs, y=val_labels)\n","        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n","        \n","        hd_metric(y_pred=val_outputs, y=val_labels)\n","        hd_metric_batch(y_pred=val_outputs, y=val_labels)\n","        \n","        meanIoU_metric(y_pred=val_outputs, y=val_labels)\n","        meanIoU_metric_batch(y_pred=val_outputs, y=val_labels)\n","        \n","        sd_metric(y_pred=val_outputs, y=val_labels)\n","        sd_metric_batch(y_pred=val_outputs, y=val_labels)\n","        \n","        surfaceDice_metric(y_pred=val_outputs, y=val_labels)\n","        surfaceDice_metric_batch(y_pred=val_outputs, y=val_labels)\n","        \n","        \n","\n","    metric_org = dice_metric.aggregate().item()\n","    metric_batch_org = dice_metric_batch.aggregate()\n","    \n","    hd_metric_org = hd_metric.aggregate().item()\n","    hd_metric_batch_org = hd_metric_batch.aggregate()\n","    \n","    meanIoU_metric_org = meanIoU_metric.aggregate().item()\n","    meanIoU_metric_batch_org = meanIoU_metric_batch.aggregate()\n","    \n","    sd_metric_org = sd_metric.aggregate().item()\n","    sd_metric_batch_org = sd_metric_batch.aggregate()\n","    \n","    surfaceDice_metric_org = surfaceDice_metric.aggregate().item()\n","    surfaceDice_metric_batch_org = surfaceDice_metric_batch.aggregate()\n","\n","    dice_metric.reset()\n","    dice_metric_batch.reset()\n","    \n","    hd_metric.reset()\n","    hd_metric_batch.reset()\n","    \n","    meanIoU_metric.reset()\n","    meanIoU_metric_batch.reset()\n","    \n","    sd_metric.reset()\n","    sd_metric_batch.reset()\n","    \n","    surfaceDice_metric.reset()\n","    surfaceDice_metric_batch.reset()\n","print(\"infer_time: \", time.time()-infer_start)\n","    \n","metric_tc, metric_wt, metric_et = metric_batch_org[0].item(), metric_batch_org[1].item(), metric_batch_org[2].item()\n","\n","hd_metric_tc, hd_metric_wt, hd_metric_et = hd_metric_batch_org[0].item(), hd_metric_batch_org[1].item(), hd_metric_batch_org[2].item()\n","\n","meanIoU_metric_tc, meanIoU_metric_wt, meanIoU_metric_et = meanIoU_metric_batch_org[0].item(), meanIoU_metric_batch_org[1].item(), meanIoU_metric_batch_org[2].item()\n","\n","sd_metric_tc, sd_metric_wt, sd_metric_et = sd_metric_batch_org[0].item(), sd_metric_batch_org[1].item(), sd_metric_batch_org[2].item()\n","\n","surfaceDice_metric_tc, surfaceDice_metric_wt, surfaceDice_metric_et = surfaceDice_metric_batch_org[0].item(), surfaceDice_metric_batch_org[1].item(), surfaceDice_metric_batch_org[2].item()\n","\n","print(\"Metric on original image spacing: \", metric_org)\n","print(f\"metric_tc: {metric_tc:.4f}\")\n","print(f\"metric_wt: {metric_wt:.4f}\")\n","print(f\"metric_et: {metric_et:.4f}\")\n","\n","print(\"HD Metric on original image spacing: \", hd_metric_org)\n","print(f\"HD metric_tc: {hd_metric_tc:.4f}\")\n","print(f\"HD metric_wt: {hd_metric_wt:.4f}\")\n","print(f\"HD metric_et: {hd_metric_et:.4f}\")\n","\n","print(\"MeanIoU Metric on original image spacing: \", meanIoU_metric_org)\n","print(f\"MeanIoU metric_tc: {meanIoU_metric_tc:.4f}\")\n","print(f\"MeanIoU metric_wt: {meanIoU_metric_wt:.4f}\")\n","print(f\"MeanIoU metric_et: {meanIoU_metric_et:.4f}\")\n","\n","print(\"SD Metric on original image spacing: \", sd_metric_org)\n","print(f\"SD metric_tc: {sd_metric_tc:.4f}\")\n","print(f\"SD metric_wt: {sd_metric_wt:.4f}\")\n","print(f\"SD metric_et: {sd_metric_et:.4f}\")\n","\n","print(\"Surface Dice Metric on original image spacing: \", surfaceDice_metric_org)\n","print(f\"Surface Dice metric_tc: {surfaceDice_metric_tc:.4f}\")\n","print(f\"Surface Dice metric_wt: {surfaceDice_metric_wt:.4f}\")\n","print(f\"Surface Dice metric_et: {surfaceDice_metric_et:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Cleanup data directory\n","\n","Remove directory if a temporary was used."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if directory is None:\n","    shutil.rmtree(root_dir)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4173979,"sourceId":7235749,"sourceType":"datasetVersion"},{"datasetId":4174104,"sourceId":7235830,"sourceType":"datasetVersion"},{"datasetId":4394153,"sourceId":7549906,"sourceType":"datasetVersion"},{"datasetId":4343232,"sourceId":7551226,"sourceType":"datasetVersion"},{"datasetId":4566415,"sourceId":7833812,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
